---
layout: single
title:  "[LLM] 2. LLM의 핵심 구성 요소"
categories: LLM
tags: [LLM, AI, NLP, Transformer, 토큰화, 임베딩, 어텐션]
author_profile: true
toc: true
toc_sticky: true
toc_label: 목차
---

## 1. 토큰화(Tokenization)

### 1.1 토큰화란?
- **정의**: 텍스트를 의미 있는 단위로 분리하는 과정
- **목적**: 텍스트를 컴퓨터가 처리할 수 있는 형태로 변환
- **방식**: 단어 단위, 문자 단위, 서브워드 단위

### 1.2 주요 토큰화 기법
- **WordPiece**
  - BERT에서 사용되는 토큰화 방법
  - 자주 등장하는 단어는 그대로, 희귀한 단어는 서브워드로 분리
  - 예: "unhappiness" → "un", "##happiness"

- **Byte-Pair Encoding (BPE)**
  - GPT 시리즈에서 사용
  - 가장 빈번한 바이트 쌍을 반복적으로 병합
  - 데이터 기반으로 어휘를 구축

- **SentencePiece**
  - 구글에서 개발한 토큰화 방법
  - 언어에 구애받지 않는 통합 토큰화
  - 유니코드 문자를 직접 처리

## 2. 임베딩(Embedding)

### 2.1 임베딩이란?
- **정의**: 단어나 토큰을 벡터 공간으로 매핑하는 과정
- **목적**: 단어의 의미를 수치적으로 표현
- **특징**: 유사한 의미의 단어는 벡터 공간에서 가까이 위치

### 2.2 임베딩의 종류
- **Word2Vec**
  - 단어의 주변 단어를 통해 의미 학습
  - CBOW와 Skip-gram 두 가지 방식
  - 단어 간의 의미적 관계 표현

- **GloVe**
  - 전체 코퍼스의 통계적 정보 활용
  - 단어의 동시 등장 확률을 기반으로 학습
  - 전역적 통계 정보와 지역적 정보 모두 활용

- **Transformer 기반 임베딩**
  - 문맥을 고려한 동적 임베딩
  - 같은 단어라도 문맥에 따라 다른 벡터 표현
  - BERT, GPT 등에서 사용

## 3. 어텐션 메커니즘

### 3.1 어텐션의 기본 개념
- **정의**: 입력 시퀀스의 각 부분에 대한 중요도를 계산하는 메커니즘
- **목적**: 관련 있는 정보에 더 많은 가중치 부여
- **특징**: 병렬 처리 가능, 장기 의존성 문제 해결

### 3.2 어텐션의 종류
- **Self-Attention**
  - 같은 시퀀스 내의 모든 위치 간의 관계 학습
  - 각 단어가 다른 모든 단어와의 관계를 고려
  - 문맥 이해에 핵심적인 역할

- **Multi-Head Attention**
  - 여러 개의 어텐션 헤드를 병렬로 사용
  - 다양한 관점에서의 관계 학습 가능
  - 모델의 표현력 향상

- **Cross-Attention**
  - 다른 시퀀스 간의 관계 학습
  - 인코더-디코더 구조에서 주로 사용
  - 번역, 요약 등에 활용

## 4. 트랜스포머 아키텍처

### 4.1 트랜스포머의 기본 구조
- **인코더 (Encoder)**
  - 입력 시퀀스를 처리
  - Self-Attention과 Feed-Forward Network로 구성
  - 여러 레이어를 쌓아 깊은 표현 학습

- **디코더 (Decoder)**
  - 출력 시퀀스를 생성
  - Masked Self-Attention과 Cross-Attention 사용
  - 이전 출력을 기반으로 다음 토큰 예측

- **피드포워드 네트워크 (Feed-Forward Network)**
  - 각 위치에서 독립적으로 적용
  - 비선형 변환 수행
  - 모델의 표현력 향상

### 4.2 트랜스포머의 주요 구성 요소
- **Positional Encoding**
  - 토큰의 위치 정보를 벡터로 표현
  - 순서 정보를 모델에 제공
  - 사인/코사인 함수를 사용한 고정된 인코딩

- **Layer Normalization**
  - 각 레이어의 출력을 정규화
  - 학습 안정성 향상
  - 그래디언트 흐름 개선

- **Residual Connection**
  - 입력과 출력을 직접 연결
  - 그래디언트 소실 문제 해결
  - 깊은 네트워크 학습 가능

- **Masked Attention**
  - 디코더에서 미래 토큰 접근 방지
  - 자기회귀적 생성 가능
  - 시퀀스 생성의 일관성 보장

> 참고 자료
> - [Attention is All You Need](https://arxiv.org/abs/1706.03762)
> - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
> - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

> 필요한 이미지/자료
> 1. 토큰화 과정 시각화
> 2. 임베딩 공간에서의 단어 분포도
> 3. 어텐션 메커니즘 작동 방식 다이어그램
> 4. 트랜스포머 아키텍처 전체 구조도